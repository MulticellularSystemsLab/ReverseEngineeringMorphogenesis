{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import spatial_efd\n",
    "import math \n",
    "import signac\n",
    "import numpy as np\n",
    "import os.path\n",
    "import os\n",
    "import torch\n",
    "import gpytorch\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread(\"target_disc_label.png\")\n",
    "cv2.imshow(\"Cute Kitens\", img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "STEP 0:\n",
    "Load the input and output data gennerated by SE model for building a GPR model\n",
    "\n",
    "\"\"\"\n",
    "# Checking if data exists\n",
    "doesDataFileExist = os.path.isfile(\"master_feature_output.npy\")\n",
    "\n",
    "# Loading datafiles if they exist\n",
    "# Else fetching and preparing data from signac workspace\n",
    "if doesDataFileExist == True:\n",
    "    # Loading input parameters\n",
    "    master_parameter_input_n = np.load('master_parameter_input_n.npy', )\n",
    "    # Loading output EFD coefficients\n",
    "    master_feature_output = np.load('master_feature_output.npy', )\n",
    "\n",
    "# Printing shape of the daya\n",
    "print(np.shape(master_parameter_input_n))\n",
    "print(np.shape(master_feature_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "STEP 2: Data preprocessing\n",
    "1. Selects the parameters sampled in LHS from total 35 parameters of the SE model\n",
    "2. PCA on the out shape features.\n",
    "\"\"\"\n",
    "\n",
    "data_efd_mean = np.mean(master_feature_output,axis = 0)\n",
    "data_efd_variance = np.var(master_feature_output,axis = 0)\n",
    "# Loading in the data processing class\n",
    "dataPreprocess  = dataPreprocessing(master_parameter_input_n, master_feature_output, 133)\n",
    "# Converting the input parameters to logscale\n",
    "master_parameter_input_log = dataPreprocess.inputLogTransform()\n",
    "print(np.shape(master_parameter_input_log))\n",
    "# Selecting the parameters that were sampled in the latin hypercube sampling\n",
    "num_parameters_LHS = 7\n",
    "LHS_parameter_index = [1, 4, 7, 17, 18, 19, 33]\n",
    "# Calling in the function to separate out the desired parameters\n",
    "data_x = dataPreprocess.inputParameterSelection(num_parameters_LHS, LHS_parameter_index, master_parameter_input_log)\n",
    "print(np.shape(data_x))\n",
    "\n",
    "# PCA to reduce dimensionality of the output data\n",
    "total_variance_explained, principalComponents, weights = dataPreprocess.pcaEfdFeatures(8)\n",
    "print(total_variance_explained)\n",
    "print(np.shape(weights))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Step 3: Training the GPR model\n",
    "Input: Parametsr sampled in LHS for the SE model\n",
    "Output: PC1\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "data_x_mean = np.mean(data_x, axis=0)\n",
    "data_x_variance = np.std(data_x, axis=0)\n",
    "print (np.shape(data_x_mean))\n",
    "print (np.shape(data_x_variance))\n",
    "\n",
    "# Normalizing data\n",
    "data_x = StandardScaler().fit_transform(data_x)\n",
    "print (np.min(data_x, axis = 0))\n",
    "print (np.max(data_x, axis = 0))\n",
    "\n",
    "\n",
    "data_y = principalComponents[:,0]\n",
    "\n",
    "\n",
    "# To do: Incorporate it in the GPR class\n",
    "class ExactGPModel(gpytorch.models.ExactGP):\n",
    "    def __init__(self, train_x, train_y, likelihood):\n",
    "        super(ExactGPModel, self).__init__(train_x, train_y, likelihood)\n",
    "        self.mean_module = gpytorch.means.ConstantMean()\n",
    "        # Defining a RBF kernel\n",
    "        #self.covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel())\n",
    "        #defing a Matern kernel\n",
    "        # mu is the smoothness parameter\n",
    "        self.covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.MaternKernel(nu=0.5))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
    "\n",
    "# Calling in the gpr class\n",
    "gpr  = gaussianProcessRegression(data_x, data_y)\n",
    "# Splitting up the training and test data\n",
    "train_x, train_y, test_x, test_y = gpr.split_data(110, 133)\n",
    "# Getting the trained model and likelihood using the training data\n",
    "model, likelihood = gpr.GP_Regressor(train_x, train_y, test_x, test_y, 100, 1, ExactGPModel)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" STEP 4: Acquistion function:  Expected improvenet\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "from smt.sampling_methods import LHS\n",
    "\n",
    "xlimits = np.array([[-2, 2],[-2, 2],[-2, 2],[-2, 2],[-2, 2],[-2, 2],[-2, 2]])\n",
    "sampling = LHS(xlimits = xlimits)\n",
    "# Defining numvber of samples\n",
    "num_samples = 10000000\n",
    "# Implementing latin hypercube sampling\n",
    "x = sampling(num_samples)\n",
    "\"\"\"\n",
    "\n",
    "\"\"\" Temporary bypassing due to anaconda smt installation errors\"\"\"\n",
    "# Defining numvber of samples\n",
    "num_samples = 1000000\n",
    "# Implementing latin hypercube sampling\n",
    "x = np.random.rand(num_samples, 7)\n",
    "\n",
    "\n",
    "# Calling in the acquisition function class\n",
    "af = acqisitionFunctions(x, test_x, test_y)\n",
    "# Calculating the xpected improvement\n",
    "ei = af.expected_improvement(model, likelihood, 0.9)\n",
    "\n",
    "# Finding the indez that leads to maximum acquisition function\n",
    "x_sampled_index = np.argmax(ei)\n",
    "# Assessing the new sampled value\n",
    "x_sampled_logscale_standardized = x[x_sampled_index,:]\n",
    "# Converting x sampled into parameter space\n",
    "# Multiplying by standard deviation and adding the mean pf data\n",
    "x_sampled = 10**(np.add(np.multiply(x_sampled_logscale_standardized,data_x_variance), data_x_mean)) \n",
    "\n",
    "print(x_sampled)\n",
    "print(np.shape(x_sampled))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "STEP 4: Running surface evolver simulations and estimating new ysampled\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\"\"\" Step a: Creating parameter value \"\"\"\n",
    "# Initializaib=ng the surface evolver parameters\n",
    "paraminputs = [0,0.0001,0,0,0,0,0,0.001,0,0,0, 0.1,0.1,10,0.1,0.1,0.1,0.1,10,0.0001,0.001,0.001, 1,1,0.6,0.6,0.6,0.6,0.2,0.1,3,0.6,1.8, 0.001,0.001]\n",
    "# Repalcaing the parameters with newly sampled values\n",
    "\n",
    "paraminputs[1] = x_sampled[0,]\n",
    "# tension cuboidal basal\n",
    "paraminputs[4] = x_sampled[1,]\n",
    "# tension columnar basal\n",
    "paraminputs[7] = x_sampled[2,]\n",
    "# k columnar apical\n",
    "paraminputs[17] = x_sampled[3,]\n",
    "# k columnar basal\n",
    "paraminputs[18] = x_sampled[4,]\n",
    "# k columnar lateral\n",
    "paraminputs[19] = x_sampled[5,]\n",
    "# K_ECM\n",
    "paraminputs[33] = x_sampled[6,]\n",
    "\n",
    "# Defining the set system pressure\n",
    "param_pressure = 0.001\n",
    "\n",
    "\"\"\" Step b: Writing geometry file\"\"\"\n",
    "geometryWriter(paraminputs, param_pressure, 'wingDisc')\n",
    "\n",
    "\"\"\" Step c: Running surface evolver simulations\"\"\"\n",
    "os.system(\"/home/nkumar4/Desktop/evolver_installation/src/evolver wingDisc.fe\")\n",
    "os.system(\"exit\")\n",
    "\n",
    "\"\"\" Step d: Extracting EFD features \"\"\"\n",
    "fe = FeatureExtractor('vertices.txt', 'log_edges.xlsx')\n",
    "efd_coeff_sampled = fe.tissue_efd_coeff(20)\n",
    "efd_coeff_sampled_reshaped = np.reshape(efd_coeff_sampled, (80,))\n",
    "\n",
    "\"\"\" Step e: Converting efd features to PCs/ ysampled\"\"\"\n",
    "# normalizing efd coefficients with existing data mean and variance\n",
    "efd_coeff_sampled_normalized = (np.add(np.multiply(efd_coeff_sampled_reshaped,data_efd_variance), data_efd_mean)) \n",
    "efd_coeff_sampled_normalized = np.reshape(efd_coeff_sampled_normalized, (80,1))\n",
    "# Multiplying EFD coefficients by already obtained weight of pc\n",
    "efd_coeff_sampled_normalized_pc = np.matmul(weights,efd_coeff_sampled_normalized)\n",
    "# Reshaping array for appending to the original data array\n",
    "y_sampled = np.reshape(efd_coeff_sampled_normalized_pc, (1,8))\n",
    "print(np.shape(efd_coeff_sampled_normalized_pc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fe = FeatureExtractor('vertices.txt', 'log_edges.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fe.plot_tissue_shape(\"tissue_shape.svg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"STEP 5: UPDATING TRAINING DATA\"\"\"\n",
    "#data_y = np.vstack((data_y, y_sampled[0,0]))\n",
    "data_x = np.vstack((data_x, np.reshape(x_sampled,(1,7))))\n",
    "print(np.shape(data_x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
