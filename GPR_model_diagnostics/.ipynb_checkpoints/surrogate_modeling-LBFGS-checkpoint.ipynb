{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import spatial_efd\n",
    "import math \n",
    "import signac\n",
    "import numpy as np\n",
    "import os.path\n",
    "from IPython.display import Markdown, display\n",
    "def printmd(string):\n",
    "    display(Markdown(string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining number of input parameters and number of outputs in the feature \n",
    "num_samples = 150\n",
    "num_harmonics = 20\n",
    "num_input_parameter = 35"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading screening data data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150, 35)\n",
      "(150, 80)\n"
     ]
    }
   ],
   "source": [
    "\"\"\" This section of the code does the following tasks\n",
    "1. Creating input and output training data for building surrogate models\n",
    "2. The input data is stored in master_parameter_input array with shape [num_samples,num_parameters]\n",
    "3. The output data is stored in master_parameter_output array with shape [num samples, 4 x num_harmonics]\n",
    "\"\"\"\n",
    "# Fetching project from signac workspace\n",
    "\n",
    "# Checking if data exists\n",
    "doesDataFileExist = os.path.isfile(\"master_feature_output.npy\")\n",
    "\n",
    "# Loading datafiles if they exist\n",
    "# Else fetching and preparing data from signac workspace\n",
    "if doesDataFileExist == True:\n",
    "    # Loading input parameters\n",
    "    master_parameter_input_n = np.load('master_parameter_input_n.npy', )\n",
    "    # Loading output EFD coefficients\n",
    "    master_feature_output = np.load('master_feature_output.npy', )\n",
    "else:\n",
    "    print(\"No data file exists!\")\n",
    "\n",
    "\n",
    "print(np.shape(master_parameter_input_n))\n",
    "print(np.shape(master_feature_output))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Output data preprocessing - Feature reduction using PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.21895301 0.17323367 0.11290526 0.09812456 0.08294328 0.05576723\n",
      " 0.04099771 0.03003538]\n"
     ]
    }
   ],
   "source": [
    "\"\"\" A) his section of code proejects the feature space into lower dimensions using PCA\n",
    "b) Scikit learn was first used to normalize the data and then take principal components\n",
    "c) Varaince captured in the principal components is also estimated\n",
    "d) Further the section plots the correlations between KECM and different principal components\n",
    "\"\"\"\n",
    "# Importing libraries\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Storing the feature output data in x (Not save as x as x is for inputs)\n",
    "x = master_feature_output\n",
    "# Normalizing the data\n",
    "x = StandardScaler().fit_transform(x)\n",
    "# Defining number of components in PCA\n",
    "pca = PCA(n_components=8)\n",
    "# Using scikit learn to calculate PCs\n",
    "principalComponents = pca.fit_transform(x)\n",
    "# Calculating weights\n",
    "weights = pca.components_\n",
    "# Variance explained in the principal components\n",
    "print(pca.explained_variance_ratio_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Input data preprocesseing : Parameter selection and data normaization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nilay\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:17: RuntimeWarning: divide by zero encountered in log\n"
     ]
    }
   ],
   "source": [
    "\"\"\" TRSAINING AND TESTING DATA FOR GPR MODEL\n",
    "    a) This section of the code prepares the training data for the GPR model.\n",
    "    b) Parameter that were varied during the LHS rae chosen as the input variables to the model.\n",
    "    c) Output training data are the PCs of the PCs of the EFD features\n",
    "    d) A split is carried out in the inut and output data to create a training and testing dataset for model\n",
    "    e) Definition of parameters varied in LHS\n",
    "       i) param_2 - T_squamous_basal \n",
    "       ii) param_5 - T_cuboidal_basal\n",
    "       iii) param_8 - T_columnar_basal\n",
    "       iv) param_18 - k_columnar_basal\n",
    "       v) param_19 - k_columnar_apical\n",
    "       vi) param_20 - k_columnar_lateral\n",
    "       vii) param_34 - k_ecm\n",
    "\"\"\"\n",
    "\n",
    "# Transforming input parameter data to log scale\n",
    "master_parameter_input = np.log(master_parameter_input_n)\n",
    "# Number of parameters in the Latin Hypercube sampling\n",
    "num_parameters_LHS = 7\n",
    "param_index = [1, 4, 7, 17, 18, 19, 33]\n",
    "split_size = 110\n",
    "# Initializing the training data\n",
    "train_x_numpy = np.zeros((num_samples, num_parameters_LHS))\n",
    "# Getting the parameter values from master_parameter_input\n",
    "for i in range(num_parameters_LHS):\n",
    "    train_x_numpy[:,i] = master_parameter_input[:,param_index[i]]\n",
    "\n",
    "# Normalizing the data around mean\n",
    "train_x_numpy = StandardScaler().fit_transform(train_x_numpy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GPR modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Importing librarie\n",
    "\"\"\"\n",
    "\n",
    "import math\n",
    "import torch\n",
    "import gpytorch\n",
    "from matplotlib import pyplot as plt\n",
    "\"\"\" Loading LBFGS library:https://github.com/hjmshi/PyTorch-LBFGS\n",
    "\"\"\"\n",
    "from LBFGS import FullBatchLBFGS\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "\"\"\" This section of the code calculates the likelihood based on RBF Kernel\n",
    "    ExactGPModels are defined \n",
    "    A) Model 1: Input: Parameters, Output: PC1\n",
    "    B) Model 2: Input: Parameters, Output: PC2\n",
    "    \n",
    "    Code for GP regression derived from GpyTorch example: \n",
    "    https://docs.gpytorch.ai/en/stable/examples/01_Exact_GPs/Simple_GP_Regression.html\n",
    "\"\"\"\n",
    "\n",
    "# We will use the simplest form of GP model, exact inference\n",
    "class ExactGPModel(gpytorch.models.ExactGP):\n",
    "    def __init__(self, train_x, train_y, likelihood):\n",
    "        super(ExactGPModel, self).__init__(train_x, train_y, likelihood)\n",
    "        self.mean_module = gpytorch.means.ConstantMean()\n",
    "        \"\"\" Defining a RBF kernel \"\"\"\n",
    "        self.covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel(ard_num_dims = 7))\n",
    "        \"\"\" Defining a Matern kernel \"\"\"\n",
    "        # mu is the smoothness parameter\n",
    "        #self.covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.MaternKernel(nu=0.5))\n",
    "        \"\"\" Defining a cosine Kernel \"\"\"\n",
    "        #self.covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.CosineKernel())\n",
    "        \"\"\" Defining a linear kernel \"\"\"\n",
    "        #self.covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.LinearKernel())\n",
    "        \"\"\" Defining a periodic Kernel \"\"\"\n",
    "        #self.covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.PeriodicKernel())\n",
    "        \"\"\" Defining piecewise polunomial Kernel\"\"\"\n",
    "        #self.covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.PiecewisePolynomialKernel())\n",
    "        \"\"\" Defining a RQ Kernel \"\"\"\n",
    "        #self.covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RQKernel())\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
    "\n",
    "# initialize likelihood and model\n",
    "likelihood = gpytorch.likelihoods.GaussianLikelihood()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nilay\\Documents\\GitHub\\Tissue-Cartography\\GPR_model_diagnostics\\LBFGS.py:257: UserWarning: This overload of add_ is deprecated:\n",
      "\tadd_(Number alpha, Tensor other)\n",
      "Consider using one of the following signatures instead:\n",
      "\tadd_(Tensor other, *, Number alpha) (Triggered internally at  ..\\torch\\csrc\\utils\\python_arg_parser.cpp:1005.)\n",
      "  p.data.add_(step_size, update[offset:offset + numel].view_as(p.data))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 1/1000 - Loss: 3.322 \n",
      "Iter 2/1000 - Loss: 3.091 \n",
      "Iter 3/1000 - Loss: 2.746 \n",
      "Iter 4/1000 - Loss: 2.611 \n",
      "Iter 5/1000 - Loss: 2.515 \n",
      "Iter 6/1000 - Loss: 2.476 \n",
      "Iter 7/1000 - Loss: 2.439 \n",
      "Iter 8/1000 - Loss: 2.427 \n",
      "Iter 9/1000 - Loss: 2.415 \n",
      "Iter 10/1000 - Loss: 2.408 \n",
      "Iter 11/1000 - Loss: 2.382 \n",
      "Iter 12/1000 - Loss: 2.375 \n",
      "Iter 13/1000 - Loss: 2.370 \n",
      "Iter 14/1000 - Loss: 2.366 \n",
      "Iter 15/1000 - Loss: 2.362 \n",
      "Iter 16/1000 - Loss: 2.360 \n",
      "Iter 17/1000 - Loss: 2.359 \n",
      "Iter 18/1000 - Loss: 2.359 \n",
      "Iter 19/1000 - Loss: 2.358 \n",
      "Iter 20/1000 - Loss: 2.356 \n",
      "Iter 21/1000 - Loss: 2.355 \n",
      "Iter 22/1000 - Loss: 2.353 \n",
      "Iter 23/1000 - Loss: 2.352 \n",
      "Iter 24/1000 - Loss: 2.352 \n",
      "Iter 25/1000 - Loss: 2.352 \n",
      "Iter 26/1000 - Loss: 2.352 \n",
      "Iter 27/1000 - Loss: 2.352 \n",
      "Iter 28/1000 - Loss: 2.351 \n",
      "Iter 29/1000 - Loss: 2.351 \n",
      "Iter 30/1000 - Loss: 2.351 \n",
      "Iter 31/1000 - Loss: 2.350 \n",
      "Iter 32/1000 - Loss: 2.350 \n",
      "Iter 33/1000 - Loss: 2.349 \n",
      "Iter 34/1000 - Loss: 2.349 \n",
      "Iter 35/1000 - Loss: 2.349 \n",
      "Iter 36/1000 - Loss: 2.349 \n",
      "Iter 37/1000 - Loss: 2.349 \n",
      "Iter 38/1000 - Loss: 2.349 \n",
      "Iter 39/1000 - Loss: 2.349 \n",
      "Iter 40/1000 - Loss: 2.349 \n",
      "Iter 41/1000 - Loss: 2.349 \n",
      "Iter 42/1000 - Loss: 2.349 \n",
      "Iter 43/1000 - Loss: 2.349 \n",
      "Iter 44/1000 - Loss: 2.349 \n",
      "Iter 45/1000 - Loss: 2.349 \n",
      "Iter 46/1000 - Loss: 2.349 \n",
      "Iter 47/1000 - Loss: 2.349 \n",
      "Iter 48/1000 - Loss: 2.349 \n",
      "Iter 49/1000 - Loss: 2.349 \n",
      "Iter 50/1000 - Loss: 2.349 \n",
      "Iter 51/1000 - Loss: 2.349 \n",
      "Iter 52/1000 - Loss: 2.349 \n",
      "Iter 53/1000 - Loss: 2.349 \n",
      "Iter 54/1000 - Loss: 2.349 \n",
      "Iter 55/1000 - Loss: 2.349 \n",
      "Iter 56/1000 - Loss: 2.349 \n",
      "Iter 57/1000 - Loss: 2.349 \n",
      "Iter 58/1000 - Loss: 2.349 \n",
      "Iter 59/1000 - Loss: 2.349 \n",
      "Iter 60/1000 - Loss: 2.349 \n",
      "Iter 61/1000 - Loss: 2.349 \n",
      "Iter 62/1000 - Loss: 2.349 \n",
      "Iter 63/1000 - Loss: 2.349 \n",
      "Iter 64/1000 - Loss: 2.349 \n",
      "Iter 65/1000 - Loss: 2.349 \n",
      "Iter 66/1000 - Loss: 2.349 \n",
      "Iter 67/1000 - Loss: 2.349 \n",
      "Iter 68/1000 - Loss: 2.349 \n",
      "Iter 69/1000 - Loss: 2.349 \n",
      "Iter 70/1000 - Loss: 2.349 \n",
      "Iter 71/1000 - Loss: 2.349 \n",
      "Iter 72/1000 - Loss: 2.349 \n",
      "Iter 73/1000 - Loss: 2.349 \n",
      "Iter 74/1000 - Loss: 2.349 \n",
      "Iter 75/1000 - Loss: 2.349 \n",
      "Iter 76/1000 - Loss: 2.349 \n",
      "Iter 77/1000 - Loss: 2.349 \n",
      "Iter 78/1000 - Loss: 2.349 \n",
      "Iter 79/1000 - Loss: 2.349 \n",
      "Iter 80/1000 - Loss: 2.349 \n",
      "Iter 81/1000 - Loss: 2.349 \n",
      "Iter 82/1000 - Loss: 2.349 \n",
      "Iter 83/1000 - Loss: 2.349 \n",
      "Iter 84/1000 - Loss: 2.349 \n",
      "Iter 85/1000 - Loss: 2.349 \n",
      "Iter 86/1000 - Loss: 2.349 \n",
      "Iter 87/1000 - Loss: 2.349 \n",
      "Iter 88/1000 - Loss: 2.349 \n",
      "Iter 89/1000 - Loss: 2.349 \n",
      "Iter 90/1000 - Loss: 2.349 \n",
      "Iter 91/1000 - Loss: 2.349 \n",
      "Iter 92/1000 - Loss: 2.349 \n",
      "Iter 93/1000 - Loss: 2.349 \n",
      "Iter 94/1000 - Loss: 2.349 \n",
      "Iter 95/1000 - Loss: 2.349 \n",
      "Iter 96/1000 - Loss: 2.349 \n",
      "Iter 97/1000 - Loss: 2.349 \n",
      "Iter 98/1000 - Loss: 2.349 \n",
      "Iter 99/1000 - Loss: 2.349 \n",
      "Iter 100/1000 - Loss: 2.349 \n",
      "Iter 101/1000 - Loss: 2.349 \n",
      "Iter 102/1000 - Loss: 2.349 \n",
      "Iter 103/1000 - Loss: 2.349 \n",
      "Iter 104/1000 - Loss: 2.349 \n",
      "Iter 105/1000 - Loss: 2.349 \n",
      "Iter 106/1000 - Loss: 2.349 \n",
      "Iter 107/1000 - Loss: 2.349 \n",
      "Iter 108/1000 - Loss: 2.349 \n",
      "Iter 109/1000 - Loss: 2.349 \n",
      "Iter 110/1000 - Loss: 2.349 \n",
      "Iter 111/1000 - Loss: 2.349 \n",
      "Iter 112/1000 - Loss: 2.349 \n",
      "Iter 113/1000 - Loss: 2.349 \n",
      "Iter 114/1000 - Loss: 2.349 \n",
      "Iter 115/1000 - Loss: 2.349 \n",
      "Iter 116/1000 - Loss: 2.349 \n",
      "Iter 117/1000 - Loss: 2.349 \n",
      "Iter 118/1000 - Loss: 2.349 \n",
      "Iter 119/1000 - Loss: 2.349 \n",
      "Iter 120/1000 - Loss: 2.349 \n",
      "Iter 121/1000 - Loss: 2.349 \n",
      "Iter 122/1000 - Loss: 2.349 \n",
      "Iter 123/1000 - Loss: 2.349 \n",
      "Iter 124/1000 - Loss: 2.349 \n",
      "Iter 125/1000 - Loss: 2.349 \n",
      "Iter 126/1000 - Loss: 2.349 \n",
      "Iter 127/1000 - Loss: 2.349 \n",
      "Iter 128/1000 - Loss: 2.349 \n",
      "Iter 129/1000 - Loss: 2.349 \n",
      "Iter 130/1000 - Loss: 2.349 \n",
      "Iter 131/1000 - Loss: 2.349 \n",
      "Iter 132/1000 - Loss: 2.349 \n",
      "Iter 133/1000 - Loss: 2.349 \n",
      "Iter 134/1000 - Loss: 2.349 \n",
      "Iter 135/1000 - Loss: 2.349 \n",
      "Iter 136/1000 - Loss: 2.349 \n",
      "Iter 137/1000 - Loss: 2.349 \n",
      "Iter 138/1000 - Loss: 2.349 \n",
      "Iter 139/1000 - Loss: 2.349 \n",
      "Iter 140/1000 - Loss: 2.349 \n",
      "Iter 141/1000 - Loss: 2.349 \n",
      "Iter 142/1000 - Loss: 2.349 \n",
      "Iter 143/1000 - Loss: 2.349 \n",
      "Iter 144/1000 - Loss: 2.349 \n",
      "Iter 145/1000 - Loss: 2.349 \n",
      "Iter 146/1000 - Loss: 2.349 \n",
      "Iter 147/1000 - Loss: 2.349 \n",
      "Iter 148/1000 - Loss: 2.349 \n",
      "Iter 149/1000 - Loss: 2.349 \n",
      "Iter 150/1000 - Loss: 2.349 \n",
      "Iter 151/1000 - Loss: 2.349 \n",
      "Iter 152/1000 - Loss: 2.349 \n",
      "Iter 153/1000 - Loss: 2.349 \n",
      "Iter 154/1000 - Loss: 2.349 \n",
      "Iter 155/1000 - Loss: 2.349 \n",
      "Iter 156/1000 - Loss: 2.349 \n",
      "Iter 157/1000 - Loss: 2.349 \n",
      "Iter 158/1000 - Loss: 2.349 \n",
      "Iter 159/1000 - Loss: 2.349 \n",
      "Iter 160/1000 - Loss: 2.349 \n",
      "Iter 161/1000 - Loss: 2.349 \n",
      "Iter 162/1000 - Loss: 2.349 \n",
      "Iter 163/1000 - Loss: 2.349 \n",
      "Iter 164/1000 - Loss: 2.349 \n",
      "Iter 165/1000 - Loss: 2.349 \n",
      "Iter 166/1000 - Loss: 2.349 \n",
      "Iter 167/1000 - Loss: 2.349 \n",
      "Iter 168/1000 - Loss: 2.349 \n",
      "Iter 169/1000 - Loss: 2.349 \n",
      "Iter 170/1000 - Loss: 2.349 \n",
      "Iter 171/1000 - Loss: 2.349 \n",
      "Iter 172/1000 - Loss: 2.349 \n",
      "Iter 173/1000 - Loss: 2.349 \n",
      "Iter 174/1000 - Loss: 2.349 \n",
      "Iter 175/1000 - Loss: 2.349 \n",
      "Iter 176/1000 - Loss: 2.349 \n",
      "Iter 177/1000 - Loss: 2.349 \n",
      "Iter 178/1000 - Loss: 2.349 \n",
      "Iter 179/1000 - Loss: 2.349 \n",
      "Iter 180/1000 - Loss: 2.349 \n",
      "Iter 181/1000 - Loss: 2.349 \n",
      "Iter 182/1000 - Loss: 2.349 \n",
      "Iter 183/1000 - Loss: 2.349 \n",
      "Iter 184/1000 - Loss: 2.349 \n",
      "Iter 185/1000 - Loss: 2.349 \n",
      "Iter 186/1000 - Loss: 2.349 \n",
      "Iter 187/1000 - Loss: 2.349 \n"
     ]
    },
    {
     "ename": "NanError",
     "evalue": "cholesky_cpu: 12100 of 12100 elements of the torch.Size([110, 110]) tensor are NaN.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\gpytorch\\utils\\cholesky.py\u001b[0m in \u001b[0;36mpsd_safe_cholesky\u001b[1;34m(A, upper, out, jitter, max_tries)\u001b[0m\n\u001b[0;32m     31\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m         \u001b[0mL\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcholesky\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mA\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mupper\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mupper\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     33\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mL\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: cholesky_cpu: U(1,1) is zero, singular U.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mNanError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-9bc438c94adc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     67\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtraining_iter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m         \u001b[0moptions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m'closure'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mclosure\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'current_loss'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'max_ls'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 69\u001b[1;33m         \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mF_eval\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mG_eval\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     70\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Iter %d/%d - Loss: %.3f '\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining_iter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\optim\\optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     87\u001b[0m                 \u001b[0mprofile_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"Optimizer.step#{}.step\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     88\u001b[0m                 \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprofile_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 89\u001b[1;33m                     \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     90\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     91\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\GitHub\\Tissue-Cartography\\GPR_model_diagnostics\\LBFGS.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, options)\u001b[0m\n\u001b[0;32m   1104\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1105\u001b[0m         \u001b[1;31m# take step\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1106\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Documents\\GitHub\\Tissue-Cartography\\GPR_model_diagnostics\\LBFGS.py\u001b[0m in \u001b[0;36m_step\u001b[1;34m(self, p_k, g_Ok, g_Sk, options)\u001b[0m\n\u001b[0;32m    822\u001b[0m             \u001b[1;31m# update and evaluate at new point\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    823\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_add_update\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0md\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 824\u001b[1;33m             \u001b[0mF_new\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclosure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    825\u001b[0m             \u001b[0mclosure_eval\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    826\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-7-9bc438c94adc>\u001b[0m in \u001b[0;36mclosure\u001b[1;34m()\u001b[0m\n\u001b[0;32m     57\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m-\u001b[0m\u001b[0mmll\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     60\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     61\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\gpytorch\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0m_validate_module_outputs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0moutput\u001b[0m \u001b[1;32min\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\gpytorch\\mlls\\exact_marginal_log_likelihood.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, function_dist, target, *params)\u001b[0m\n\u001b[0;32m     60\u001b[0m         \u001b[1;31m# Get the log prob of the marginal distribution\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     61\u001b[0m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlikelihood\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunction_dist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 62\u001b[1;33m         \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog_prob\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     63\u001b[0m         \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_add_other_terms\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mres\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\gpytorch\\distributions\\multivariate_normal.py\u001b[0m in \u001b[0;36mlog_prob\u001b[1;34m(self, value)\u001b[0m\n\u001b[0;32m    167\u001b[0m         \u001b[1;31m# Get log determininant and first part of quadratic form\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    168\u001b[0m         \u001b[0mcovar\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcovar\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate_kernel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 169\u001b[1;33m         \u001b[0minv_quad\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogdet\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcovar\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minv_quad_logdet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minv_quad_rhs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdiff\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogdet\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    170\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    171\u001b[0m         \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m0.5\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0minv_quad\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogdet\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdiff\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mmath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mmath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\gpytorch\\lazy\\lazy_tensor.py\u001b[0m in \u001b[0;36minv_quad_logdet\u001b[1;34m(self, inv_quad_rhs, logdet, reduce_inv_quad)\u001b[0m\n\u001b[0;32m   1246\u001b[0m                     \u001b[0mwill_need_cholesky\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1247\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mwill_need_cholesky\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1248\u001b[1;33m                 \u001b[0mcholesky\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCholLazyTensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mTriangularLazyTensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcholesky\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1249\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mcholesky\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minv_quad_logdet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minv_quad_rhs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minv_quad_rhs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogdet\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlogdet\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreduce_inv_quad\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mreduce_inv_quad\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1250\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\gpytorch\\lazy\\lazy_tensor.py\u001b[0m in \u001b[0;36mcholesky\u001b[1;34m(self, upper)\u001b[0m\n\u001b[0;32m    965\u001b[0m             \u001b[1;33m(\u001b[0m\u001b[0mLazyTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[0mCholesky\u001b[0m \u001b[0mfactor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtriangular\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mupper\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mlower\u001b[0m \u001b[0mdepending\u001b[0m \u001b[0mon\u001b[0m \u001b[1;34m\"upper\"\u001b[0m \u001b[0marg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    966\u001b[0m         \"\"\"\n\u001b[1;32m--> 967\u001b[1;33m         \u001b[0mchol\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_cholesky\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mupper\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    968\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mupper\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    969\u001b[0m             \u001b[0mchol\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mchol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_transpose_nonbatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\gpytorch\\utils\\memoize.py\u001b[0m in \u001b[0;36mg\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     57\u001b[0m         \u001b[0mkwargs_pkl\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdumps\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0m_is_in_cache\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcache_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs_pkl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mkwargs_pkl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0m_add_to_cache\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcache_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs_pkl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mkwargs_pkl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     60\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0m_get_from_cache\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcache_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs_pkl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mkwargs_pkl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     61\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\gpytorch\\lazy\\lazy_tensor.py\u001b[0m in \u001b[0;36m_cholesky\u001b[1;34m(self, upper)\u001b[0m\n\u001b[0;32m    427\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    428\u001b[0m         \u001b[1;31m# contiguous call is necessary here\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 429\u001b[1;33m         \u001b[0mcholesky\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpsd_safe_cholesky\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mevaluated_mat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mupper\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mupper\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    430\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mTriangularLazyTensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcholesky\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mupper\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mupper\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    431\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\gpytorch\\utils\\cholesky.py\u001b[0m in \u001b[0;36mpsd_safe_cholesky\u001b[1;34m(A, upper, out, jitter, max_tries)\u001b[0m\n\u001b[0;32m     36\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misnan\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0many\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m             raise NanError(\n\u001b[1;32m---> 38\u001b[1;33m                 \u001b[1;34mf\"cholesky_cpu: {isnan.sum().item()} of {A.numel()} elements of the {A.shape} tensor are NaN.\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     39\u001b[0m             )\n\u001b[0;32m     40\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNanError\u001b[0m: cholesky_cpu: 12100 of 12100 elements of the torch.Size([110, 110]) tensor are NaN."
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 900x300 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\" To do: 1. Calculate the final value of the hyperparameters \n",
    "        2. How are the hyperparameters initaialized? Print out the initialvalues of the hyperparameters\n",
    "        3. Find an example of using LBFGS in GPyTorch and inmplement it in our code. The adam optimizer may not be doing a good job.\n",
    "        4. Try one lengthscale per feature for the RBF kernel instead of one gloibal lengthscale\n",
    "        5. A dictionary kind of a data sctructure for how the modelparameter evolves over the tarining iterations\n",
    "        6. Surrogate modeling v2 and geberalize these functions (remove the reduntant parts)\n",
    "        7. Initial values of the hyperparameters are 0 right now. Confirm with documentation. (Especially parameters like lengthscale ofor covariance kernel.)\n",
    "\"\"\"\n",
    "\n",
    "# Defines the number of samples used for training and testing the gpr model\n",
    "split_size = 110\n",
    "num_pc_analyzed = 2\n",
    "\n",
    "from matplotlib.pyplot import figure\n",
    "figure(figsize=(9, 3), dpi=100)\n",
    "\n",
    "\"\"\" Looping through the first two principal components\"\"\"\n",
    "\n",
    "for j in range(num_pc_analyzed):\n",
    "    \"\"\" Training data \"\"\"\n",
    "    # Converting numpy array to tensor\n",
    "    train_x = torch.from_numpy(train_x_numpy[:split_size,:])\n",
    "    # Converting the output training data to numpy array\n",
    "    train_y = torch.from_numpy(principalComponents[:split_size,j])\n",
    "    \"\"\" Testing data \"\"\"\n",
    "    test_x = torch.from_numpy(train_x_numpy[split_size:num_samples,:])\n",
    "    test_y = principalComponents[split_size:num_samples,j]\n",
    "    # Defining models for GPR\n",
    "    model = ExactGPModel(train_x, train_y, likelihood)\n",
    "    #print(\"Model-\"+str(j+1) + \" Initial Hyperparameters\")\n",
    "    #for param_name, param in model.named_parameters():\n",
    "        #print(f'Parameter name: {param_name:42} value = {param.item()}')\n",
    "    hypers = {\n",
    "        'covar_module.base_kernel.lengthscale': torch.tensor([[1, 1, 1, 1, 1, 1, 1]]),\n",
    "    }\n",
    "    model.initialize(**hypers)\n",
    "        \n",
    "    \"\"\" Training the GPR model\"\"\"\n",
    "    # this is for running the notebook in our testing framework\n",
    "    import os\n",
    "    smoke_test = ('CI' in os.environ)\n",
    "    training_iter = 2 if smoke_test else 1000\n",
    "    # Find optimal model hyperparameters\n",
    "    model.train()\n",
    "    likelihood.train()\n",
    "    # Use the adam optimizer\n",
    "    #optimizer = torch.optim.Adam(model.parameters(), lr=0.1)  # Includes GaussianLikelihood parameters\n",
    "    # \n",
    "    optimizer =  FullBatchLBFGS(model.parameters())\n",
    "    \n",
    "    # \"Loss\" for GPs - the marginal log likelihood\n",
    "    mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n",
    "    \n",
    "    \"\"\" Reference for using LBFGS  \n",
    "    https://github.com/hjmshi/PyTorch-LBFGS/blob/master/examples/Gaussian_Processes/gp_regression.py\"\"\"\n",
    "    def closure():\n",
    "        optimizer.zero_grad()\n",
    "        output = model(train_x)\n",
    "        loss = -mll(output, train_y)\n",
    "        return loss\n",
    "    \n",
    "    loss = closure()\n",
    "    loss.backward()\n",
    "    \n",
    "    training_iter = 1000\n",
    "    \n",
    "    for i in range(training_iter):\n",
    "        options = {'closure': closure, 'current_loss': loss, 'max_ls': 10}\n",
    "        loss, _, lr, _, F_eval, G_eval, _, _ = optimizer.step(options)\n",
    "\n",
    "        print('Iter %d/%d - Loss: %.3f ' % (i + 1, training_iter, loss.item()))\n",
    "\n",
    "    #print(\"Model-\"+str(j+1) + \" Trained Hyperparameters\")\n",
    "    #for param_name, param in model.named_parameters():\n",
    "        #print(f'Parameter name: {param_name:42} value = {param.item()}')\n",
    "    \n",
    "    # Get into evaluation (predictive posterior) mode\n",
    "    model.eval()\n",
    "    likelihood.eval()\n",
    "    # Test points are regularly spaced along [0,1]\n",
    "    # Make predictions by feeding model through likelihood\n",
    "    with torch.no_grad(), gpytorch.settings.fast_pred_var():\n",
    "        observed_pred = likelihood(model(test_x))\n",
    "        \n",
    "    with torch.no_grad():\n",
    "        # Calculating upper and lower bounds of model predictions\n",
    "        lower, upper = observed_pred.confidence_region()\n",
    "        # converting upper and lower bound prediction sto numpy array\n",
    "        lower_numpy = lower.numpy()\n",
    "        upper_numpy = upper.numpy()\n",
    "        # Claculating mean prediction\n",
    "        output_model_predictions = observed_pred.mean.numpy()\n",
    "        # fetching actual output data\n",
    "        original_output = test_y\n",
    "        \n",
    "    # Calculating total error in predictions \n",
    "    error_prediction = np.subtract(upper_numpy, lower_numpy)\n",
    "    # Discretizing coordinate system for updating the parietal_plots\n",
    "    x_par = np.linspace(np.amin(original_output),np.amax(original_output), num = 100)\n",
    "    # Plotting the parietal line y = x\n",
    "    plt.subplot(1,2,j+1)\n",
    "    plt.plot(x_par, x_par)\n",
    "    # Plotting the output predictions against known output value\n",
    "    plt.plot(original_output, output_model_predictions, 'o', color='black')\n",
    "    # Plotting the errorbars\n",
    "    plt.errorbar(original_output, output_model_predictions,\n",
    "                yerr = error_prediction, lolims = lower_numpy, uplims = upper_numpy, linestyle = \"None\")\n",
    "    plt.xlabel(\"True PC-\" + str(j+1),color=\"red\")\n",
    "    plt.ylabel(\"Predicted PC-\" + str(j+1),color=\"red\")\n",
    "        \n",
    "plt.show()            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.covar_module.base_kernel.lengthscale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
