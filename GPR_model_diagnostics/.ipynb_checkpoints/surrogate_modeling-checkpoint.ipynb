{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import spatial_efd\n",
    "import math \n",
    "import signac\n",
    "import numpy as np\n",
    "import os.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining number of input parameters and number of outputs in the feature \n",
    "num_samples = 150\n",
    "num_harmonics = 20\n",
    "num_input_parameter = 35"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading screening data data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150, 35)\n",
      "(150, 80)\n"
     ]
    }
   ],
   "source": [
    "\"\"\" This section of the code does the following tasks\n",
    "1. Creating input and output training data for building surrogate models\n",
    "2. The input data is stored in master_parameter_input array with shape [num_samples,num_parameters]\n",
    "3. The output data is stored in master_parameter_output array with shape [num samples, 4 x num_harmonics]\n",
    "\"\"\"\n",
    "# Fetching project from signac workspace\n",
    "\n",
    "# Checking if data exists\n",
    "doesDataFileExist = os.path.isfile(\"master_feature_output.npy\")\n",
    "\n",
    "# Loading datafiles if they exist\n",
    "# Else fetching and preparing data from signac workspace\n",
    "if doesDataFileExist == True:\n",
    "    # Loading input parameters\n",
    "    master_parameter_input_n = np.load('master_parameter_input_n.npy', )\n",
    "    # Loading output EFD coefficients\n",
    "    master_feature_output = np.load('master_feature_output.npy', )\n",
    "else:\n",
    "    print(\"No data file exists!\")\n",
    "\n",
    "\n",
    "print(np.shape(master_parameter_input_n))\n",
    "print(np.shape(master_feature_output))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Output data preprocessing - Feature reduction using PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.21895301 0.17323367 0.11290526 0.09812456 0.08294328 0.05576723\n",
      " 0.04099771 0.03003538]\n"
     ]
    }
   ],
   "source": [
    "\"\"\" A) his section of code proejects the feature space into lower dimensions using PCA\n",
    "b) Scikit learn was first used to normalize the data and then take principal components\n",
    "c) Varaince captured in the principal components is also estimated\n",
    "d) Further the section plots the correlations between KECM and different principal components\n",
    "\"\"\"\n",
    "# Importing libraries\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Storing the feature output data in x\n",
    "x = master_feature_output\n",
    "# Normalizing the data\n",
    "x = StandardScaler().fit_transform(x)\n",
    "# Defining number of components in PCA\n",
    "pca = PCA(n_components=8)\n",
    "# Using scikit learn to calculate PCs\n",
    "principalComponents = pca.fit_transform(x)\n",
    "# Calculating weights\n",
    "weights = pca.components_\n",
    "# Variance explained in the principal components\n",
    "print(pca.explained_variance_ratio_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Input data preprocesseing : Parameter selection and data normaization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nilay\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:17: RuntimeWarning: divide by zero encountered in log\n"
     ]
    }
   ],
   "source": [
    "\"\"\" TRSAINING AND TESTING DATA FOR GPR MODEL\n",
    "    a) This section of the code prepares the training data for the GPR model.\n",
    "    b) Parameter that were varied during the LHS rae chosen as the input variables to the model.\n",
    "    c) Output training data are the PCs of the PCs of the EFD features\n",
    "    d) A split is carried out in the inut and output data to create a training and testing dataset for model\n",
    "    e) Definition of parameters varied in LHS\n",
    "       i) param_2 - T_squamous_basal \n",
    "       ii) param_5 - T_cuboidal_basal\n",
    "       iii) param_8 - T_columnar_basal\n",
    "       iv) param_18 - k_columnar_basal\n",
    "       v) param_19 - k_columnar_apical\n",
    "       vi) param_20 - k_columnar_lateral\n",
    "       vii) param_34 - k_ecm\n",
    "\"\"\"\n",
    "\n",
    "# Transforming input parameter data to log scale\n",
    "master_parameter_input = np.log(master_parameter_input_n)\n",
    "# Number of parameters in the Latin Hypercube sampling\n",
    "num_parameters_LHS = 7\n",
    "param_index = [1, 4, 7, 17, 18, 19, 33]\n",
    "split_size = 110\n",
    "pc_index_anal = 7\n",
    "# Initializing the training data\n",
    "train_x_numpy = np.zeros((num_samples, num_parameters_LHS))\n",
    "# Getting the parameter values from master_parameter_input\n",
    "for i in range(num_parameters_LHS):\n",
    "    train_x_numpy[:,i] = master_parameter_input[:,param_index[i]]\n",
    "\n",
    "# Normalizing the data around mean\n",
    "train_x_numpy = StandardScaler().fit_transform(train_x_numpy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GPR modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Importing librarie\n",
    "\"\"\"\n",
    "\n",
    "import math\n",
    "import torch\n",
    "import gpytorch\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "\"\"\" This section of the code calculates the likelihood based on RBF Kernel\n",
    "    ExactGPModels are defined \n",
    "    A) Model 1: Input: Parameters, Output: PC1\n",
    "    B) Model 2: Input: Parameters, Output: PC2\n",
    "    \n",
    "    Code for GP regression derived from GpyTorch example: \n",
    "    https://docs.gpytorch.ai/en/stable/examples/01_Exact_GPs/Simple_GP_Regression.html\n",
    "\"\"\"\n",
    "\n",
    "# We will use the simplest form of GP model, exact inference\n",
    "class ExactGPModel(gpytorch.models.ExactGP):\n",
    "    def __init__(self, train_x, train_y, likelihood):\n",
    "        super(ExactGPModel, self).__init__(train_x, train_y, likelihood)\n",
    "        self.mean_module = gpytorch.means.ConstantMean()\n",
    "        \"\"\" Defining a RBF kernel \"\"\"\n",
    "        #self.covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel())\n",
    "        \"\"\" Defining a Matern kernel \"\"\"\n",
    "        # mu is the smoothness parameter\n",
    "        #self.covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.MaternKernel(nu=0.5))\n",
    "        \"\"\" Defining a cosine Kernel \"\"\"\n",
    "        #self.covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.CosineKernel())\n",
    "        \"\"\" Defining a linear kernel \"\"\"\n",
    "        #self.covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.LinearKernel())\n",
    "        \"\"\" Defining a periodic Kernel \"\"\"\n",
    "        #self.covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.PeriodicKernel())\n",
    "        \"\"\" Defining piecewise polunomial Kernel\"\"\"\n",
    "        #self.covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.PiecewisePolynomialKernel())\n",
    "        \"\"\" Defining a RQ Kernel \"\"\"\n",
    "        self.covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RQKernel())\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
    "\n",
    "# initialize likelihood and model\n",
    "likelihood = gpytorch.likelihoods.GaussianLikelihood()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defines the number of samples used for training and testing the gpr model\n",
    "split_size = 110\n",
    "num_pc_analyzed = 2\n",
    "\n",
    "from matplotlib.pyplot import figure\n",
    "figure(figsize=(9, 3), dpi=100)\n",
    "\n",
    "\"\"\" Looping through the first two principal components\"\"\"\n",
    "\n",
    "for j in range(num_pc_analyzed):\n",
    "    \"\"\" Training data \"\"\"\n",
    "    # Converting numpy array to tensor\n",
    "    train_x = torch.from_numpy(train_x_numpy[:split_size,:])\n",
    "    # Converting the output training data to numpy array\n",
    "    train_y = torch.from_numpy(principalComponents[:split_size,j])\n",
    "    \"\"\" Testing data \"\"\"\n",
    "    test_x = torch.from_numpy(train_x_numpy[split_size:num_samples,:])\n",
    "    test_y = principalComponents[split_size:num_samples,j]\n",
    "    # Defining models for GPR\n",
    "    model = ExactGPModel(train_x, train_y, likelihood)\n",
    "    \"\"\" Training the GPR model\"\"\"\n",
    "    # this is for running the notebook in our testing framework\n",
    "    import os\n",
    "    smoke_test = ('CI' in os.environ)\n",
    "    training_iter = 2 if smoke_test else 1000\n",
    "    # Find optimal model hyperparameters\n",
    "    model.train()\n",
    "    likelihood.train()\n",
    "    # Use the adam optimizer\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.1)  # Includes GaussianLikelihood parameters\n",
    "    # \"Loss\" for GPs - the marginal log likelihood\n",
    "    mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n",
    "    \n",
    "    for i in range(training_iter):\n",
    "        # Zero gradients from previous iteration\n",
    "        optimizer.zero_grad()\n",
    "        # Output from model\n",
    "        output = model(train_x)\n",
    "        # Calc loss and backprop gradients\n",
    "        loss = -mll(output, train_y)\n",
    "        loss.backward()\n",
    "        print('Iter %d/%d - Loss: %.3f' % (\n",
    "            i + 1, training_iter, loss.item(),\n",
    "\n",
    "        ))\n",
    "        optimizer.step()\n",
    "    \n",
    "    # Get into evaluation (predictive posterior) mode\n",
    "    model.eval()\n",
    "    likelihood.eval()\n",
    "    # Test points are regularly spaced along [0,1]\n",
    "    # Make predictions by feeding model through likelihood\n",
    "    with torch.no_grad(), gpytorch.settings.fast_pred_var():\n",
    "        observed_pred = likelihood(model(test_x))\n",
    "        \n",
    "    with torch.no_grad():\n",
    "        # Calculating upper and lower bounds of model predictions\n",
    "        lower, upper = observed_pred.confidence_region()\n",
    "        # converting upper and lower bound prediction sto numpy array\n",
    "        lower_numpy = lower.numpy()\n",
    "        upper_numpy = upper.numpy()\n",
    "        # Claculating mean prediction\n",
    "        output_model_predictions = observed_pred.mean.numpy()\n",
    "        # fetching actual output data\n",
    "        original_output = test_y\n",
    "        \n",
    "    # Calculating total error in predictions \n",
    "    error_prediction = np.subtract(upper_numpy, lower_numpy)\n",
    "    # Discretizing coordinate system for updating the parietal_plots\n",
    "    x_par = np.linspace(np.amin(original_output),np.amax(original_output), num = 100)\n",
    "    # Plotting the parietal line y = x\n",
    "    plt.subplot(1,2,j+1)\n",
    "    plt.plot(x_par, x_par)\n",
    "    # Plotting the output predictions against known output value\n",
    "    plt.plot(original_output, output_model_predictions, 'o', color='black')\n",
    "    # Plotting the errorbars\n",
    "    plt.errorbar(original_output, output_model_predictions,\n",
    "                yerr = error_prediction, lolims = lower_numpy, uplims = upper_numpy, linestyle = \"None\")\n",
    "    plt.xlabel(\"True PC-\" + str(j+1),color=\"red\")\n",
    "    plt.ylabel(\"Predicted PC-\" + str(j+1),color=\"red\")\n",
    "        \n",
    "plt.show()            "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
