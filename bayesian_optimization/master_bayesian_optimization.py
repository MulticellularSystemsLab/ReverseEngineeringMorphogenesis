# -*- coding: utf-8 -*-
"""
Created on Tue Jul  6 23:35:16 2021

@author: Nilay
"""
# Importing libraries
import pandas as pd
import matplotlib.pyplot as plt
import spatial_efd
import math 
import signac
import numpy as np
import os.path
import os
import torch
import gpytorch
import subprocess
import gc
import similaritymeasures
from matplotlib import pyplot as plt
from sklearn.preprocessing import StandardScaler
from smt.sampling_methods import LHS
# Importing helper libraries for bayesian optimization
from dependencies.data_preprocessing_class import dataPreprocessing
from dependencies.gaussian_process_regression_class import gaussianProcessRegression
from dependencies.acquisition_functions_class import acqisitionFunctions
from dependencies.geometry_writer import geometryWriter
from dependencies.feature_extractor_4 import FeatureExtractor

"""
STEP 1:
Load the input and output data generated by parameter screening of a Surface Evolver model.
This data will be used to build the surrogate model, Gaussian Process Regression (GPR) model
to be used furing the bayesian optimization process
"""
# Checking if data exists
doesDataFileExist = os.path.isfile("master_feature_output.npy")
# Loading datafiles if they exist
# Else fetching and preparing data from signac workspace
if doesDataFileExist == True:
	# Input parameter values
	master_parameter_input_n = np.load('master_parameter_input_n.npy', )
	# Output shape features - EFD descriptors of the output basal surface 
	master_feature_output = np.load('master_feature_output.npy', )
	
"""
STEP 2: Input data preprocessing - Preparing inputs for the GPR model
2.1 Select the parameters sampled in LHS from total 35 parameters of the SE model.
2.2 Normalize the parameters wrt the medan and standard deviatio of the screened parameters.
"""
# Loading in the data processing class
dataPreprocess  = dataPreprocessing(master_parameter_input_n, master_feature_output, 150)
# Converting the input parameters to logscale
master_parameter_input_log = dataPreprocess.inputLogTransform()
# Selecting the parameters that were sampled in the latin hypercube sampling
num_parameters_LHS = 7
LHS_parameter_index = [17, 18, 19, 28, 29, 30, 33]
# Calling in the function to separate out the desired parameters
data_x = dataPreprocess.inputParameterSelection(num_parameters_LHS, LHS_parameter_index, master_parameter_input_log)
# Storing mean and standard deviation of input training data for later use
data_x_mean = np.mean(data_x, axis=0)
data_x_variance = np.std(data_x, axis=0)
# Normalizing data
data_x = StandardScaler().fit_transform(data_x)
# Calculating the minimum and maximum for input parameters for the purpose of sampling the parameters using LHS later
max_data_x = np.amax(data_x, axis=0) 
min_data_x = np.amin(data_x, axis=0) 


"""
Step 3: Preparing output data for the GPR model 
2.1 Shape for whicb parameter estimation has to be carried out is loaded first
2.2 EFD and reverse EFD is used to obtain the x,y points of the basal surface which is normalized wrt orientation and translation.
2.3 Frachet distance is evaluated betwen the shaped shampled during parameter screening and the target shape.
    Frechet error is calculated based on the difference in shapes of the basal surfaces. 	
"""

# Reading in experimental data as a list of xy points representing the tissue lateral shape
# Commented out for benchmarking the pipeline against a synthetic input generated using SE

"""
a. calc_method == 1: the geometry file is a text file containg the xy coordinates of of the basal surface
b. calc_method == 2: output file of a surface evolver simulation (used for benchmarking)
"""
# Input the type of geometry file you will be inputting as target shape data
calc_method = 2
#geometry_data_target_disc = 'vertices_target.txt'
geometry_data_target_disc = 'input_data/vertices4.txt'

if calc_method == 1:
	# Check if the filename is correct
	if type(geometry_data_target_disc) is str:
		# Check if the text file contains data
		if os.stat(geometry_data_target_disc).st_size != 0:
			a1 = []
			a2 = []
			with open(geometry_data_target_disc) as f:
				# Reading individual line
				for line in f:
					data = line.split()
					# appending the xy coordinates into an array
					a1.append(float(data[0]))
					a2.append(float(data[1]))
					
		else:
			a1 = 0
			a2 = 0
	# Creating and storing the arrays containing x and y coordinates of basal surface		
	vpos_x_exp = a1
	vpos_y_exp = a2
	# Extracting EFD coefficient from the basal surface
	coeff_exp = spatial_efd.CalculateEFD(vpos_x_exp, vpos_y_exp, 20)
	# Normalizing the coefficients against rotation and size
	coeff_exp, rotation = spatial_efd.normalize_efd(coeff_exp, size_invariant=True)
elif calc_method == 2:
	# Reading the vertices output file from a sample SE simulation output with known parameters
	fe_exp = FeatureExtractor(geometry_data_target_disc, 'log_edges.xlsx')
	# Extracting the efd coefficients
	coeffs_exp, dummy_1, dummy_2 = fe_exp.tissue_efd_coeff(20)

# Extracting  x and y coordinates of the normalized basal surface using reverse EFD
xt_exp, yt_exp = spatial_efd.inverse_transform(coeffs_exp, harmonic=20)
exp_data = np.zeros((300,2))
exp_data[:,0] = xt_exp
exp_data[:,1] = yt_exp

# Initializing an array to store Frechet error
error_simulation_experimental_data = np.zeros(150)

# Iterating through all the sample in parameter screening
for i in range(150):
	# Extracting EFD coefficient of the sampled shapes (basal surfaces)
	temp = master_feature_output[i,:]
	temp2 = np.reshape(temp, (20,4))
	# Applying reverse EFD to get x,y coordinates of the normalized basal surface
	xt, yt = spatial_efd.inverse_transform(temp2, harmonic=20)
	# Arranging the x y coordinates
	sim_data = np.zeros((300,2))
	sim_data[:,0] = xt
	sim_data[:,1] = yt
	# Calling in similaritymeasures to evaluate frechet distance between the target and the shape in parameter screening
	error_simulation_experimental_data[i] = similaritymeasures.frechet_dist(exp_data,sim_data) 
    
# Multiplying errors by -1 to turn them into GPR model outputs
data_y = (np.reshape(error_simulation_experimental_data, (150,1)))*(-1)
print(np.shape(data_y))
			 
"""
Step 4: class ExactGP Model 
        Needs to be executed to execute the 
		  gaussianProcessRegression class 
        Training the Gaussian process regression model
"""
class ExactGPModel(gpytorch.models.ExactGP):
	def __init__(self, train_x, train_y, likelihood):
		super(ExactGPModel, self).__init__(train_x, train_y, likelihood)
		# Defining the kernel for mean
		self.mean_module = gpytorch.means.ConstantMean()
		# Using the kernel with different lengthscales in different dimensions
		self.covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel(ard_num_dims=7))
		# self.white_noise = gpytorch.kernels.ScaleKernel(gpytorch.kernels.WhiteNoiseKernel())
		
	def forward(self, x):
		mean_x = self.mean_module(x)
		covar_x = self.covar_module(x)
		return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)


# Preparing the traing data for GPR. An all vs one methodology is applied 
# where all the datapoints exceppt the last one has been chosen fro training
split_size = 149
num_samples = 150
train_x = data_x[:split_size,:]
train_y = data_y[:split_size,:]
test_x = data_x[split_size:num_samples,:]
test_y = data_y[split_size:num_samples,:]

"""
Step 5: 
a) Sampling the parameter space for clculation of acquisition function
"""
# defining the limits of sampling
xlimits = np.array([[min_data_x[0], max_data_x[0]],[min_data_x[1], max_data_x[1]],[min_data_x[2], max_data_x[2]],[min_data_x[3], max_data_x[3]],[min_data_x[4], max_data_x[4]],[min_data_x[5], max_data_x[5]],[min_data_x[6], max_data_x[6]]])
# Caling Latine Hypercube Sampling to sample the parameters within the above defined limits
sampling = LHS(xlimits = xlimits)
# Number of samples. These will be used for calculation of acquisition function
num_samples = 100000
# sample
x = sampling(num_samples)

"""
STep 5: Starting the main BO loop
5.1 Training the GPR model
5.2 Calculating expected imrovement and finidning the new point
5.3 Running surface evolver for the sampled parameters
5.4 Extract new input and output for the GPR model
5.5 Add to the GPR training data and Go back to 1
"""
# Defining the total number of iterations in the BO loop
n_iterations = 100
# Initializing the array that will store the error of sampled shapes from the target shape
error_target_sampled = []
iter_counter = []
# Initializing the array to store the parameters sampleed during each iteration BO process
param_sampled = np.zeros((n_iterations,7))

for i in range(n_iterations):
	""" 5.1 Training the GPR model"""	
	gpr = gaussianProcessRegression(train_x, train_y, test_x, test_y)
	model, likelihood, train_x_t, train_y_t, test_x_t, test_y_pc = gpr.GP_model_definition(ExactGPModel,0,1)
	model, likelihood, lengthscale_hyperparameters = gpr.GP_training(train_x_t, train_y_t,model,likelihood,1, 5000)
	
	""" 5.2 Calculating expected imrovement and finding the new sampling point"""
	# Calculating expected improvement
	af = acqisitionFunctions(x, train_x, train_y)
	ei, model_prediction_mean, model_prediction_variance = af.expected_improvement(model, likelihood, 0.05)
	
	# Using EI to sample a new point
	x_sampled_index = np.argmax(ei)
	x_sampled_logscale_standardized = x[x_sampled_index,:]
	# Converting x sampled into parameter space
	x_sampled = np.exp(np.add(np.multiply(x_sampled_logscale_standardized,data_x_variance), data_x_mean))
	
	""" 5.3 Running surface evolver for the sampled parameters"""
	# Initializaib=ng the surface evolver parameters
	paraminputs = [0,0.0001,0,0,0,0,0,0.001,0,0,0, 0.1,0.1,10,0.1,0.1,0.1,0.1,10,0.0001,0.001,0.001, 1,1,0.6,0.6,0.6,0.6,0.2,0.1,3,0.6,1.8, 0.001,0.001]
	# Repalcaing the parameters with newly sampled values
	paraminputs[17] = x_sampled[0,]
	# tension cuboidal basal
	paraminputs[18] = x_sampled[1,]
	# tension columnar basal
	paraminputs[19] = x_sampled[2,]
	# k columnar apical
	paraminputs[28] = x_sampled[3,]
	# k columnar basal
	paraminputs[29] = x_sampled[4,]
	# k columnar lateral
	paraminputs[30] = x_sampled[5,]
	# K_ECM
	paraminputs[33] = x_sampled[6,]
	# Defining the set system pressure
	param_pressure = 0.001
	# Writing geometry file
	geometryWriter(paraminputs, param_pressure, 'wingDisc')
	# Running surface evolver simulations
	# Can be replaced using python subprocess using 
	os.system("/home/nkumar4/Desktop/evolver_installation/src/evolver wingDisc.fe")
	
	""" 5.4 Extracting features drom the sampled shape"""
	# Calling the feature extractor class to extract featuresfrom output vertices.txt file
	fe = FeatureExtractor('vertices.txt', 'log_edges.xlsx')
	# Calculating the EFD coefficients
	efd_coeff_sampled, dummy_3, dummy_4 = fe.tissue_efd_coeff(20)
	# Calculating the xy cordinates of the normalized basal surface of the sampled shape
	xt_sampled, yt_sampled = spatial_efd.inverse_transform(efd_coeff_sampled, harmonic=20)
	# Saving the vertices file
	command_save_vertices = "cp vertices.txt vertices_" + str(i) + ".txt"
	os.system(command_save_vertices)
	# Remove the additional files created by surface evolver
	os.system("rm vertices.txt")
	os.system("rm energylog.txt")
	os.system("rm specificenergylog.txt")
	
	# Defining filename for plot showing overlap between the sampled shape and the target shape
	filename_shape_plot = str(i) + "_sapled_target_xy_plot.svg"
	# Plotting target data
	plt.plot(xt_exp,yt_exp,'black', label='Target')
	# Plotting sampled data
	plt.plot(xt_sampled, yt_sampled,'blue', label='Sampled')
	plt.axes().set_aspect('equal', 'datalim')
	# Labeling axes
	plt.xlabel("x [nondimensional]")
	plt.ylabel("y [nondimensional]")
	# Plotting legends
	plt.legend()
	plt.savefig("contour_evolution_plots/" + filename_shape_plot)
	plt.close()
	
	""" 5.5 Adding the new sampled data to the GPR model"""
	# Calculating error: Calculating frechet distance between the sampled shape and the experimental data
	sampled_data = np.zeros((300,2))
	sampled_data[:,0] = xt_sampled
	sampled_data[:,1] = yt_sampled
	error_target_sampled_step = similaritymeasures.frechet_dist(exp_data,sampled_data)
	# Multiplying the error with -1 to create the output data for GPR model
	y_sampled = np.reshape(error_target_sampled_step, (1,1))*(-1)
	# Adding to the training data 
	train_x = np.vstack((train_x, np.reshape(x_sampled_logscale_standardized,(1,7))))
	train_y = np.vstack((train_y, y_sampled))
	
	# Adding the error of the sampled datapoint to the master error array
	error_target_sampled.append(error_target_sampled_step)
	# Adding the iteration number to the counter
	iter_counter.append(i+1)
	# Adding the sampled parameter to the master sampled parameter array
	param_sampled[i,:] = x_sampled
	
	# Plotting the error sampled along with the rror in training data
	# defining the filename
	filename_error_iteration = str(i) + "error_evolution.png"
	# defining the training error which is -ve of the voutput of GPR model
	error_train = data_y*-1
	error_train_reshaped = np.reshape(error_train, (150,))
	# Sorting the error in training data
	error_train_sorted = np.flip(np.sort(error_train_reshaped))
	# Defining the forst 150 indices for the training data
	index_trained = np.linspace(1, 150, 150)
	# Defining the indices of the sampled data error
	index_sampled = np.linspace(151,150+i+1,i+1)
	# Plotting the error in training data
	plt.scatter(index_trained, error_train_sorted, color="black")
	# Plotting the error in sampled data
	plt.scatter(index_sampled, error_target_sampled, color="red")
	# Labelinf axes
	plt.ylabel("Index")
	plt.xlabel("Error between target and SE shape")
	# saving the plot in error_sampled_plot folder
	plt.savefig("error_sampled_plots/" + filename_shape_plot)
	plt.close()
	
	# Deleting variables generated within the BO loop
	del x_sampled
	del xt_sampled
	del yt_sampled
	del fe
	del gpr
	del model
	del likelihood
	del index_trained
	del index_sampled
	gc.collect()
	
	
"""
Saving important arrays
"""	
np.save('ei.npy',ei)
np.save('x.npy',x)
np.save('error_target_sampled.npy',error_target_sampled)
np.save('model_prediction_mean.npy',model_prediction_mean)
np.save('model_prediction_variance.npy',model_prediction_variance)
np.save('param_sampled.npy',param_sampled)
np.save('train_x.npy', train_x)
np.save('train_y.npy', train_y)
