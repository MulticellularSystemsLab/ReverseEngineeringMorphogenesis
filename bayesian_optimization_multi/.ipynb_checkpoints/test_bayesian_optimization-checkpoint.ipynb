{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Prototyping Bayesian Optimization code*\n",
    "\n",
    "1. INPUT: Takes in boundary points of a target wing disc shape\n",
    "2. OUTPUT: Spits out corresponding Surface Evolver modeling parameters resulting in the target shape\n",
    "3. A test case for a single iteration. A complete implementation can be found in master_bayesian_optimization.py file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import spatial_efd\n",
    "import math \n",
    "import signac\n",
    "import numpy as np\n",
    "import os.path\n",
    "import os\n",
    "import torch\n",
    "import gpytorch\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import gc\n",
    "# Importing helper libraries for bayesian optimization\n",
    "from dependencies.data_preprocessing_class import dataPreprocessing\n",
    "from dependencies.gaussian_process_regression_class import gaussianProcessRegression\n",
    "from dependencies.acquisition_functions_class import acqisitionFunctions\n",
    "from dependencies.geometry_writer import geometryWriter\n",
    "from dependencies.feature_extractor_4 import FeatureExtractor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### STEP 1\n",
    "1. Loading Input and output data generated using parameter screening \n",
    "2. Data preprocessing\n",
    "\n",
    "    a. Transforming input parameters into log scale<br>\n",
    "    b. Normalizing transformed input data<br>\n",
    "    c. Normaling the EFD output data<br>\n",
    "    c. Taking PC of the output EFD features for dimensionality reduction<br>\n",
    "    e. Selecting Principal component 1 as the output\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(133, 35)\n",
      "(133, 80)\n"
     ]
    }
   ],
   "source": [
    "# Checking if data exists\n",
    "doesDataFileExist = os.path.isfile(\"master_feature_output.npy\")\n",
    "\n",
    "# Loading datafiles if they exist\n",
    "# Else fetching and preparing data from signac workspace\n",
    "if doesDataFileExist == True:\n",
    "    # Loading input parameters\n",
    "    master_parameter_input_n = np.load('master_parameter_input_n.npy', )\n",
    "    # Loading output EFD coefficients\n",
    "    master_feature_output = np.load('master_feature_output.npy', )\n",
    "\n",
    "# Printing shape of the daya\n",
    "print(np.shape(master_parameter_input_n))\n",
    "print(np.shape(master_feature_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.00000000e+00 -2.76754803e-19  5.80933467e-18  3.42554689e-01\n",
      " -3.36692091e-03  5.04448513e-02 -1.53920024e-01  3.75995929e-03\n",
      "  6.68438137e-02  8.19851531e-04 -1.53214419e-03  1.12106414e-01\n",
      "  3.31577552e-05  2.08928305e-02 -4.17752060e-03 -1.46670356e-03\n",
      "  2.97181673e-02 -2.25045217e-04  5.09745966e-04  2.39267618e-02\n",
      " -5.95402946e-05  3.37261858e-03 -2.92267727e-03  6.05960381e-04\n",
      "  2.21606330e-03  7.51224791e-05 -5.09978159e-04  2.42044959e-03\n",
      " -2.10668077e-04 -1.08677673e-04 -1.91295464e-03 -3.11016836e-04\n",
      " -1.76325268e-03  2.17188843e-05  1.85252196e-04 -7.38423269e-04\n",
      " -1.70643207e-04 -3.35810679e-04 -5.33373540e-04  3.01050739e-04\n",
      " -8.83231410e-04  3.98459979e-05 -2.67858200e-04 -3.01117971e-04\n",
      " -2.55933403e-04 -6.07551552e-04 -1.50421751e-04 -8.06691424e-05\n",
      " -3.77398624e-04 -9.64304780e-05  8.80573889e-05  1.40610440e-04\n",
      " -1.09575349e-04 -3.67602910e-04  3.15161949e-04  4.15247662e-05\n",
      " -3.66956272e-04 -2.60787812e-05  2.08523249e-05  3.34352101e-04\n",
      " -1.33508313e-04 -2.70658575e-04  2.45444174e-04 -9.10186200e-05\n",
      " -2.94508832e-04 -9.20296902e-05  2.23505333e-05  9.30652427e-05\n",
      " -5.41129548e-05 -1.28295346e-04  3.37027151e-04 -9.05488685e-05\n",
      " -2.47629496e-04 -3.37981135e-05 -4.92110079e-06 -4.60149294e-05\n",
      " -7.64247107e-05 -5.11886717e-05  9.92849419e-05  6.16405815e-07]\n",
      "[0.00000000e+00 6.09831599e-17 6.38169945e-17 4.17557769e-02\n",
      " 4.61163166e-03 3.63560862e-02 2.29846542e-01 6.91021519e-03\n",
      " 1.93421053e-02 1.60724384e-03 3.42790883e-03 2.04694269e-02\n",
      " 1.57426227e-03 1.50417943e-02 2.35259151e-02 2.89360546e-03\n",
      " 2.13940901e-02 1.11063265e-03 1.76678408e-03 1.04006364e-02\n",
      " 1.05777281e-03 7.67339592e-03 7.62699507e-03 1.78295391e-03\n",
      " 9.11465226e-03 8.77112818e-04 1.36615261e-03 6.06427224e-03\n",
      " 1.16645163e-03 5.60300739e-03 4.18298833e-03 1.18366164e-03\n",
      " 6.11482055e-03 8.20991801e-04 8.87751980e-04 4.40351406e-03\n",
      " 1.19941682e-03 4.54552679e-03 4.35799010e-03 9.92984556e-04\n",
      " 4.67592287e-03 8.19074686e-04 8.77032103e-04 3.44778076e-03\n",
      " 1.17955042e-03 3.59345218e-03 3.19820484e-03 6.81463552e-04\n",
      " 3.75166590e-03 7.84600517e-04 7.52455695e-04 3.16898761e-03\n",
      " 1.07732106e-03 2.75343678e-03 2.10329645e-03 4.21653905e-04\n",
      " 2.86072155e-03 7.41040228e-04 5.51942824e-04 2.08829840e-03\n",
      " 9.45721297e-04 2.12368215e-03 1.88055036e-03 4.63915045e-04\n",
      " 2.19616978e-03 6.94108111e-04 4.29189638e-04 1.46396217e-03\n",
      " 7.75877809e-04 1.61196658e-03 1.30705411e-03 4.05594452e-04\n",
      " 1.65747211e-03 6.13183389e-04 2.86662052e-04 1.25947755e-03\n",
      " 6.16582819e-04 1.16458437e-03 1.06483890e-03 3.41229303e-04]\n",
      "[1.00000000e-33 6.09831599e-17 6.38169945e-17 4.17557769e-02\n",
      " 4.61163166e-03 3.63560862e-02 2.29846542e-01 6.91021519e-03\n",
      " 1.93421053e-02 1.60724384e-03 3.42790883e-03 2.04694269e-02\n",
      " 1.57426227e-03 1.50417943e-02 2.35259151e-02 2.89360546e-03\n",
      " 2.13940901e-02 1.11063265e-03 1.76678408e-03 1.04006364e-02\n",
      " 1.05777281e-03 7.67339592e-03 7.62699507e-03 1.78295391e-03\n",
      " 9.11465226e-03 8.77112818e-04 1.36615261e-03 6.06427224e-03\n",
      " 1.16645163e-03 5.60300739e-03 4.18298833e-03 1.18366164e-03\n",
      " 6.11482055e-03 8.20991801e-04 8.87751980e-04 4.40351406e-03\n",
      " 1.19941682e-03 4.54552679e-03 4.35799010e-03 9.92984556e-04\n",
      " 4.67592287e-03 8.19074686e-04 8.77032103e-04 3.44778076e-03\n",
      " 1.17955042e-03 3.59345218e-03 3.19820484e-03 6.81463552e-04\n",
      " 3.75166590e-03 7.84600517e-04 7.52455695e-04 3.16898761e-03\n",
      " 1.07732106e-03 2.75343678e-03 2.10329645e-03 4.21653905e-04\n",
      " 2.86072155e-03 7.41040228e-04 5.51942824e-04 2.08829840e-03\n",
      " 9.45721297e-04 2.12368215e-03 1.88055036e-03 4.63915045e-04\n",
      " 2.19616978e-03 6.94108111e-04 4.29189638e-04 1.46396217e-03\n",
      " 7.75877809e-04 1.61196658e-03 1.30705411e-03 4.05594452e-04\n",
      " 1.65747211e-03 6.13183389e-04 2.86662052e-04 1.25947755e-03\n",
      " 6.16582819e-04 1.16458437e-03 1.06483890e-03 3.41229303e-04]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nkumar4/Desktop/bayesian_optimization_multi/dependencies/data_preprocessing_class.py:42: RuntimeWarning: divide by zero encountered in log\n",
      "  master_parameter_input = np.log(self.master_parameter_input_n)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "data_efd_mean = np.mean(master_feature_output,axis = 0)\n",
    "print(data_efd_mean)\n",
    "data_efd_variance = np.std(master_feature_output,axis = 0)\n",
    "print(data_efd_variance)\n",
    "data_efd_variance[0,] = 10**-33\n",
    "print(data_efd_variance)\n",
    "# Loading in the data processing class\n",
    "dataPreprocess  = dataPreprocessing(master_parameter_input_n, master_feature_output, 133)\n",
    "# Converting the input parameters to logscale\n",
    "master_parameter_input_log = dataPreprocess.inputLogTransform()\n",
    "\n",
    "# Selecting the parameters that were sampled in the latin hypercube sampling\n",
    "num_parameters_LHS = 7\n",
    "LHS_parameter_index = [1, 4, 7, 17, 18, 19, 33]\n",
    "# Calling in the function to separate out the desired parameters\n",
    "data_x = dataPreprocess.inputParameterSelection(num_parameters_LHS, LHS_parameter_index, master_parameter_input_log)\n",
    "\n",
    "\n",
    "# PCA to reduce dimensionality of the output data\n",
    "total_variance_explained, principalComponents, weights, weights_af = dataPreprocess.pcaEfdFeatures(8)\n",
    "\n",
    "data_x_mean = np.mean(data_x, axis=0)\n",
    "data_x_variance = np.std(data_x, axis=0)\n",
    "\n",
    "\n",
    "# Normalizing data\n",
    "data_x = StandardScaler().fit_transform(data_x)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.          0.75216795 -1.08024297  1.11075852  0.47547595 -1.19967053\n",
      "  1.10045732 -1.08297095  0.49090498 -0.86845551  1.05495297  1.47788899\n",
      "  0.04500868 -0.8842792  -0.8790828   1.17882731 -0.80360646  0.29877076\n",
      " -1.37880963 -1.17159546 -0.12745999  0.83428777  0.00422269 -1.14958305\n",
      " -0.72446233 -0.20300411  1.21206762 -0.29973492  0.06735282  0.53489167\n",
      "  0.18792774  1.10571763 -0.19811786 -0.18825431 -1.09128313 -0.39455329\n",
      "  0.10508231  0.48968872  0.21691441 -0.99015103 -0.1730472  -0.08450309\n",
      "  1.04938089 -0.25919342  0.13594339  0.49877192  0.03190021  0.90148685\n",
      " -0.04677344  0.04941939 -0.6618466  -0.26590885  0.08428024  0.47235724\n",
      " -0.28978822 -1.00225826  0.07005857  0.07099854  0.58208516 -0.3815408\n",
      "  0.07937393  0.33737349 -0.24199658  0.74028599  0.20424563  0.10775298\n",
      " -0.54144728 -0.09170787  0.09531838  0.18355539 -0.61304878 -0.28683552\n",
      "  0.27695506  0.13016934  0.57142126 -0.16119131  0.10059289 -0.049814\n",
      " -0.15856291  0.2813207 ]\n",
      "[-4.3307239  -1.3928108  -1.5024429   0.9863069   0.13386285 -0.34782774\n",
      "  0.68376225  1.07724699]\n",
      "[-4.3307239  -1.3928108  -1.5024429   0.9863069   0.13386285 -0.34782774\n",
      "  0.68376225  1.07724699]\n"
     ]
    }
   ],
   "source": [
    "data_norm = (np.divide(np.subtract(master_feature_output[0,:],data_efd_mean), data_efd_variance))\n",
    "data_norm[0] = 0\n",
    "print(data_norm)\n",
    "pca_recon = np.matmul(weights,data_norm)\n",
    "print(principalComponents[0,:])\n",
    "print(pca_recon)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### STEP 2: Training the GPR model\n",
    "1. Split training and test data\n",
    "2. Input to GPR model: Log transformed and normalized SE parameters\n",
    "3. Output to GPR model: PC1 of the normalized EFD features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"STEP 1\"\"\"\n",
    "\"\"\"\n",
    "# Geometry file containing the boundary point for the wing disc shape for which the papameters have to be estimated\n",
    "geometry_data = 'input_data/vertices_target.txt' \n",
    "if type(geometry_data) is str:\n",
    "    # Checking if the file containing vertices coordinates are empty\n",
    "    if os.stat(geometry_data).st_size != 0:\n",
    "        # \n",
    "        a1 = []\n",
    "        a2 = []\n",
    "        \n",
    "        with open(geometry_data) as f:\n",
    "            # next(f)\n",
    "            for line in f:\n",
    "                data = line.split()\n",
    "                a1.append(float(data[0]))\n",
    "                a2.append(float(data[1]))\n",
    "                \n",
    "\n",
    "    else:\n",
    "        a1 = 0\n",
    "        a2 = 0\n",
    "        \n",
    "# vposx_exp and vpos_y_exp contains the x and y coordinates of the experimental data\n",
    "vpos_x_exp = a1\n",
    "vpos_y_exp = a2\n",
    "\n",
    "# EFD ciefficients are extracted using the spatial EFD package and boundary points of the target shape\n",
    "coeffs = spatial_efd.CalculateEFD(vpos_x_exp, vpos_y_exp, 20)\n",
    "# Normalizing the coefficients against rotation and size\n",
    "coeffs_exp, rotation = spatial_efd.normalize_efd(coeffs, size_invariant=True)\n",
    "# Reverse EFD for plotting the normalized tissue shape\n",
    "xt, yt = spatial_efd.inverse_transform(coeffs, harmonic=20)\n",
    "\n",
    "# Plotting the experimental tissue contour\n",
    "plt.plot(xt,yt,'black')\n",
    "plt.axes().set_aspect('equal', 'datalim')\n",
    "plt.xlabel(\"x [nondimensional]\")\n",
    "plt.ylabel(\"y [nondimensional]\")\n",
    "plt.show()\n",
    "\n",
    "# Reshaping the EFD features (20x4) as a horizontal array for using it as (80) features\n",
    "efd_coeff_exp_reshaped = np.reshape(coeffs_exp, (80,))\n",
    "print(efd_coeff_exp_reshaped)\n",
    "# normalizing efd coefficients with existing data mean and variance\n",
    "efd_coeff_exp_normalized = (np.add(np.multiply(efd_coeff_exp_reshaped,data_efd_variance), data_efd_mean)) \n",
    "efd_coeff_exp_normalized = np.reshape(efd_coeff_exp_normalized, (80,1))\n",
    "# Multiplying EFD coefficients by already obtained weight of pc\n",
    "efd_coeff_exp_normalized_pc = np.matmul(weights,efd_coeff_exp_normalized)\n",
    "# Reshaping array for appending to the original data array\n",
    "y_exp = np.reshape(efd_coeff_exp_normalized_pc, (1,8))\n",
    "\"\"\"\n",
    "# Reading the vertices output file from a sample SE simulation output with known parameters\n",
    "fe_exp = FeatureExtractor('input_data/vertices_target_SE.txt', 'log_edges.xlsx')\n",
    "# Extracting the efd coefficients\n",
    "coeff_exp = fe_exp.tissue_efd_coeff(20)\n",
    "# Reverse EFD for plotting the normalized tissue shape\n",
    "xt_exp, yt_exp = spatial_efd.inverse_transform(coeff_exp, harmonic=20)\n",
    "efd_coeff_exp_reshaped = np.reshape(coeff_exp, (80,))\n",
    "\n",
    "efd_coeff_exp_normalized = (np.divide(np.subtract(efd_coeff_exp_reshaped,data_efd_mean), data_efd_variance)) \n",
    "efd_coeff_exp_normalized = np.reshape(efd_coeff_exp_normalized, (80,1))\n",
    "# Multiplying EFD coefficients by already obtained weight of pc\n",
    "efd_coeff_exp_normalized_pc = np.matmul(weights,efd_coeff_exp_normalized)\n",
    "# Reshaping array for appending to the original data array\n",
    "y_exp = np.reshape(efd_coeff_exp_normalized_pc, (1,8))\n",
    "\n",
    "\n",
    "# To do: Incorporate it in the GPR class\n",
    "class ExactGPModel(gpytorch.models.ExactGP):\n",
    "    def __init__(self, train_x, train_y, likelihood):\n",
    "        super(ExactGPModel, self).__init__(train_x, train_y, likelihood)\n",
    "        self.mean_module = gpytorch.means.ConstantMean()\n",
    "        # Defining a RBF kernel\n",
    "        #self.covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel())\n",
    "        #defing a Matern kernel\n",
    "        # mu is the smoothness parameter\n",
    "        self.covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.MaternKernel(nu=0.5))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxIter = 1\n",
    "num_pc_components = 3\n",
    "error_target_sampled = []\n",
    "iter_counter = []\n",
    "param_sampled = np.zeros((maxIter,7))\n",
    "\n",
    "for i in range(maxIter):\n",
    "\t\"\"\" Step 5a: Training the GP model \n",
    "\t\"\"\"\t\n",
    "\t\"\"\" Step 5b: Estimating acquisition function\n",
    "\t\t\tSample random points in space\n",
    "\t\t\tUse the gpr model to estimate expected improvement\n",
    "\t\t\tfind optimum x\n",
    "\t\t\tTransform to parameter space\n",
    "\t\"\"\"\n",
    "\t# Method I: latine hypercube sampling\n",
    "\txlimits = np.array([[min_data_x[0], max_data_x[0]],[min_data_x[1], max_data_x[1]],[min_data_x[2], max_data_x[2]],[min_data_x[3], max_data_x[3]],[min_data_x[4], max_data_x[4]],[min_data_x[5], max_data_x[5]],[min_data_x[6], max_data_x[6]]])\n",
    "\tsampling = LHS(xlimits = xlimits)\n",
    "\t# Defining numvber of samples\n",
    "\tnum_samples = 1000000\n",
    "\t# Implementing latin hypercube sampling\n",
    "\tx = sampling(num_samples)\n",
    "\t\n",
    "\t# Method II: Random sampling\n",
    "\t#num_samples = 1000000\n",
    "\t#x = np.random.rand(num_samples, 7)\n",
    "\t\n",
    "\tei = np.zeros((num_samples,))\n",
    "\t\n",
    "\tfor j in range(num_pc_components):\n",
    "\t\t# Getting the trained model and likelihood using the training data\n",
    "\t\tmodel, likelihood = gpr.GP_Regressor(train_x, train_y, test_x, test_y, 1000, i, ExactGPModel,j)\n",
    "\t\ty_target = efd_coeff_exp_normalized_pc[j]\n",
    "\t\t# Calling in the acquisition function class\n",
    "\t\taf = acqisitionFunctions(x, test_x, test_y[:,j])\n",
    "\t\t# Calculating the xpected improvement\n",
    "\t\tei = ei + weights_af[j]*af.expected_improvement_modified(model, likelihood, y_target)\n",
    "\t\tdel model\n",
    "\t\tdel likelihood\n",
    "\t\tdel af\n",
    "\t\tgc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### STEP 3: Acquisition function for sampling a new point\n",
    "1. Extracts EFD coeeficients of a target image Takes PC of the EFD coefficients to get a target value of PC1\n",
    "2. A large number of points are sampled within the parameter space and normalized.\n",
    "3. Calculates expected improvement over sampled points in the parameter space\n",
    "4. A new point is sampled using the defined acquistion function and it is then transformed to the parameter space.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" STEP 2\"\"\"\n",
    "\"\"\"\n",
    "from smt.sampling_methods import LHS\n",
    "\n",
    "xlimits = np.array([[-2, 2],[-2, 2],[-2, 2],[-2, 2],[-2, 2],[-2, 2],[-2, 2]])\n",
    "sampling = LHS(xlimits = xlimits)\n",
    "# Defining numvber of samples\n",
    "num_samples = 10000000\n",
    "# Implementing latin hypercube sampling\n",
    "x = sampling(num_samples)\n",
    "\"\"\"\n",
    "\"\"\" Temporary bypassing due to anaconda smt installation errors\"\"\"\n",
    "# Defining numvber of samples\n",
    "num_samples = 1000000\n",
    "# Implementing latin hypercube sampling\n",
    "x = np.random.rand(num_samples, 7)\n",
    "ei = np.zeros((num_samples,))\n",
    "data_y = principalComponents\n",
    "\n",
    "    # Calling in the gpr class\n",
    "gpr  = gaussianProcessRegression(data_x, data_y)\n",
    "# Splitting up the training and test data\n",
    "train_x, train_y, test_x, test_y = gpr.split_data(110, 133)\n",
    "\n",
    "for i in range(8):\n",
    "    # PC1 is selected as the output data \n",
    "    \n",
    "\n",
    "\n",
    "    # Getting the trained model and likelihood using the training data\n",
    "    model, likelihood = gpr.GP_Regressor(train_x, train_y, test_x, test_y, 100, 1, ExactGPModel,i)\n",
    "\n",
    "    y_target = efd_coeff_exp_normalized_pc[i]\n",
    "\n",
    "    \"\"\"STEP 3\"\"\"\n",
    "    \"\"\"\n",
    "    #ACQUISITION FUNCTION OLD\n",
    "    # Calling in the acquisition function class\n",
    "    af = acqisitionFunctions(x, test_x, test_y)\n",
    "    # Calculating the xpected improvement\n",
    "    ei = af.expected_improvement(model, likelihood, 0.9)\n",
    "    \"\"\"\n",
    "    # Calling in the acquisition function class\n",
    "    af = acqisitionFunctions(x, test_x, test_y)\n",
    "    # Calculating the xpected improvement\n",
    "    ei = ei + weights_af[i]*af.expected_improvement_modified(model, likelihood, y_target)\n",
    "    print(np.shape(ei))\n",
    "    del model\n",
    "    del likelihood\n",
    "    del af\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"STEP 4\"\"\"\n",
    "param_sampling = np.zeros((10,7))\n",
    "# Finding the indez that leads to maximum acquisition function\n",
    "x_sampled_index = np.argmax(ei)\n",
    "# Assessing the new sampled value\n",
    "x_sampled_logscale_standardized = x[x_sampled_index,:]\n",
    "# Converting x sampled into parameter space\n",
    "# Multiplying by standard deviation and adding the mean pf data\n",
    "x_sampled = np.exp(np.add(np.multiply(x_sampled_logscale_standardized,data_x_variance), data_x_mean)) \n",
    "param_sampling[1,:] = x_sampled \n",
    "\n",
    "print(param_sampling)\n",
    "print(np.shape(x_sampled))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### STEP 4: Obtaining y for the new point sampled\n",
    "1. Run surface evolver on the new sampled parameter\n",
    "2. Extract EFD coefficients from the output shape\n",
    "3. Transform EFD to PC space and obtain the new ysampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Step 1\"\"\"\n",
    "# Initializaib=ng the surface evolver parameters\n",
    "paraminputs = [0,0.0001,0,0,0,0,0,0.001,0,0,0, 0.1,0.1,10,0.1,0.1,0.1,0.1,10,0.0001,0.001,0.001, 1,1,0.6,0.6,0.6,0.6,0.2,0.1,3,0.6,1.8, 0.001,0.001]\n",
    "# Repalcaing the parameters with newly sampled values\n",
    "\n",
    "paraminputs[1] = x_sampled[0,]\n",
    "# tension cuboidal basal\n",
    "paraminputs[4] = x_sampled[1,]\n",
    "# tension columnar basal\n",
    "paraminputs[7] = x_sampled[2,]\n",
    "# k columnar apical\n",
    "paraminputs[17] = x_sampled[3,]\n",
    "# k columnar basal\n",
    "paraminputs[18] = x_sampled[4,]\n",
    "# k columnar lateral\n",
    "paraminputs[19] = x_sampled[5,]\n",
    "# K_ECM\n",
    "paraminputs[33] = x_sampled[6,]\n",
    "\n",
    "# Defining the set system pressure\n",
    "param_pressure = 0.001\n",
    "\n",
    "# Writing geometry file\n",
    "geometryWriter(paraminputs, param_pressure, 'wingDisc')\n",
    "\n",
    "# Running surface evolver simulations\n",
    "os.system(\"/home/nkumar4/Desktop/evolver_installation/src/evolver wingDisc.fe\")\n",
    "os.system(\"exit\")\n",
    "\n",
    "\"\"\" Step 2 \"\"\"\n",
    "fe = FeatureExtractor('vertices.txt', 'log_edges.xlsx')\n",
    "efd_coeff_sampled = fe.tissue_efd_coeff(20)\n",
    "efd_coeff_sampled_reshaped = np.reshape(efd_coeff_sampled, (80,))\n",
    "\n",
    "\"\"\" Step 3 \"\"\"\n",
    "# normalizing efd coefficients with existing data mean and variance\n",
    "efd_coeff_sampled_normalized = (np.add(np.multiply(efd_coeff_sampled_reshaped,data_efd_variance), data_efd_mean)) \n",
    "efd_coeff_sampled_normalized = np.reshape(efd_coeff_sampled_normalized, (80,1))\n",
    "# Multiplying EFD coefficients by already obtained weight of pc\n",
    "efd_coeff_sampled_normalized_pc = np.matmul(weights,efd_coeff_sampled_normalized)\n",
    "# Reshaping array for appending to the original data array\n",
    "y_sampled = np.reshape(efd_coeff_sampled_normalized_pc, (1,8))\n",
    "print(np.shape(efd_coeff_sampled_normalized_pc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"STEP 5: UPDATING TRAINING DATA\"\"\"\n",
    "#data_y = np.vstack((data_y, y_sampled[0,0]))\n",
    "data_x = np.vstack((data_x, np.reshape(x_sampled,(1,7))))\n",
    "print(np.shape(data_x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
